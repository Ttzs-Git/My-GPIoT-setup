"""
ä½¿ç”¨LoRAæŠ€æœ¯ç»™Llama-2-13bè¿›è¡Œå¾®è°ƒ
"""

# In[]
## åŸºç¡€æ“ä½œ: å¯¼åŒ…+è®¾ç½®åœ°å€
# å¯¼åŒ…
import os
import torch
from datasets import load_from_disk # ç”¨äºåŠ è½½é¢„å¤„ç†å·çš„æ•°æ®é›†
from trl import SFTTrainer # ç”¨äºç›‘ç£å¾®è°ƒçš„é«˜çº§è®­ç»ƒå™¨
from peft import LoraConfig, prepare_model_for_kbit_training # LoRAæ ¸å¿ƒåº“
from transformers import ( #  Hugging Face ç”Ÿæ€æ ¸å¿ƒ
    AutoModelForCausalLM, # è‡ªåŠ¨åŠ è½½å› æœè¯­è¨€æ¨¡å‹
    AutoTokenizer, # è‡ªåŠ¨åŠ è½½åˆ†è¯å™¨
    BitsAndBytesConfig, # é‡åŒ–é…ç½®
    TrainingArguments, # è®­ç»ƒå‚æ•°é…ç½®
)


# å®šä¹‰è·¯å¾„(åŸºç¡€æ¨¡å‹çš„è·¯å¾„ï¼Œæ•°æ®é›†çš„è·¯å¾„å’Œè¾“å‡ºçš„è·¯å¾„)
base_model = "meta-llama/Llama-2-13b-chat-hf" # åŸºç¡€æ¨¡å‹
# new_model = "GPIoT_Code_Generation"
new_model = "GPIoT_Task_Decomposition" # æ–°æ¨¡å‹ä¿å­˜è·¯å¾„
final_dir = new_model+"/final_checkpoint" # æœ€ç»ˆcheckpointè·¯å¾„
os.makedirs(final_dir, exist_ok=True)# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨

# dataset = load_from_disk("dataset/Code_Generation_dataset")
dataset = load_from_disk("dataset/Task_Decomposition_dataset")# è®°è½½æ•°æ®é›†

# ç”¨äºæµ‹è¯•
dataset = dataset.select(range(100))  # åªå–å‰100ä¸ªæ ·æœ¬
print(f"âš ï¸ è­¦å‘Šï¼šå·²å°†æ•°æ®é›†ç¼©å‡ä¸º {len(dataset)} ä¸ªæ ·æœ¬ï¼Œä»…ç”¨äºæµ‹è¯•ï¼")

# æ£€æŸ¥æ•°æ®é›†ç±»å‹
if isinstance(dataset, dict) or "test" in dataset:
    eval_dataset = dataset["test"]
else:
    eval_dataset = None  # æˆ–åˆ’åˆ†ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†
    print("âš ï¸ è­¦å‘Š: æ•°æ®é›†æ²¡æœ‰ 'test' splitï¼Œå°†ä¸ä½¿ç”¨éªŒè¯é›†ã€‚")

# In[]
## æ¨¡å‹çš„é…ç½®ä¸é‡åŒ–

compute_dtype = getattr(torch, "float16") # é…ç½®è®¡ç®—æ•°æ®ç±»å‹
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True, # å¯ç”¨8bitåŠ è½½
    bnb_8bit_quant_type="nf8", # ä½¿ç”¨â€œnf8â€é‡åŒ–æ ¼å¼ï¼Œå¯¹LLMæ•ˆæœæ›´å¥½
    bnb_8bit_compute_dtype=compute_dtype, # è®¡ç®—æ—¶ä½¿ç”¨float16
    bnb_8bit_use_double_quant=False, # ä¸ä½¿ç”¨åŒé‡é‡åŒ–
)# é…ç½® 8-bité‡åŒ–

model = AutoModelForCausalLM.from_pretrained(
    base_model,# åŸºç¡€æ¨¡å‹
    quantization_config=quantization_config,# é‡åŒ–é…ç½®
    device_map="auto",# è‡ªåŠ¨å°†æ¨¡å‹å±‚é…ç½®åˆ°å¯ç”¨GPU
)# åŠ è½½å¹¶é‡åŒ–åŸºç¡€æ¨¡å‹

model.config.use_cache = False # å…³é—­ç¼“å­˜ï¼Œå› ä¸ºè®­ç»ƒæ—¶æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼šä¸ä¹‹å†²çª
model = prepare_model_for_kbit_training(model) # å…³é—­ç¼“å­˜ï¼Œå› ä¸ºè®­ç»ƒæ—¶æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼šä¸ä¹‹å†²çª

# In[]
## LoRAä¸è®­ç»ƒå‚æ•°é…ç½®

tokenizer = AutoTokenizer.from_pretrained(base_model)
tokenizer.pad_token = tokenizer.eos_token # å°†å¡«å……ç¬¦è®¾ä¸ºç»“æŸç¬¦
tokenizer.padding_side = "right" # å³å¡«å……ï¼Œè¿™æ˜¯å› æœè¯­è¨€æ¨¡å‹çš„å¸¸è§„åšæ³•

peft_parameters = LoraConfig(
    r=64,# ä½ç§©çŸ©é˜µçš„ç§©ï¼Œå†³å®šé€‚é…å™¨å®¹é‡ï¼ˆè¶Šå¤§è¶Šå¼ºï¼Œä½†ä¹Ÿè¶Šå æ˜¾å­˜ï¼‰
    lora_alpha=16,# ç¼©æ”¾å› å­ï¼Œæ§åˆ¶é€‚é…å™¨æ›´æ–°å¹…åº¦
    lora_dropout=0.001, # é˜²æ­¢è¿‡æ‹Ÿåˆçš„dropoutç‡
    bias="lora_only", # åªè®­ç»ƒé€‚é…å™¨éƒ¨åˆ†çš„åç½®é¡¹ï¼Œä¸è®­ç»ƒåŸå§‹æ¨¡å‹çš„åç½®
    task_type="CAUSAL_LM",# ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€å»ºæ¨¡ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰
    # target_modules=["q_proj", "v_proj"],  # â† åªåœ¨ query å’Œ value æŠ•å½±å±‚æ·»åŠ  LoRA
)# é…ç½®LoRAå‚æ•°

# é…ç½®è®­ç»ƒå‚æ•°
training_params = TrainingArguments(
    output_dir=final_dir,# æ¨¡å‹å’Œæ—¥å¿—è¾“å‡ºç›®å½•
    num_train_epochs=1,# è®­ç»ƒ1ä¸ªepoch
    per_device_train_batch_size=1, # æ¯ä¸ªGPUæ¯æ¬¡å¤„ç†1ä¸ªæ ·æœ¬ï¼ˆå› æ˜¾å­˜é™åˆ¶ï¼‰
    per_device_eval_batch_size=1,# è¯„ä¼°æ—¶åŒç†
    gradient_accumulation_steps=8,# æ¢¯åº¦ç´¯ç§¯8æ­¥ï¼Œæ¨¡æ‹Ÿ batch_size=8ï¼Œä¿è¯è®­ç»ƒç¨³å®šæ€§
    optim="paged_adamw_32bit", # noinspection SpellChecking # ä¼˜åŒ–å™¨ï¼Œé€‚åˆé‡åŒ–æ¨¡å‹ï¼Œèƒ½å¤„ç†åˆ†é¡µå†…å­˜
    save_steps=20,# æ¯20æ­¥ä¿å­˜ä¸€æ¬¡checkpoint
    logging_steps=5, # æ¯5æ­¥åœ¨æ§åˆ¶å°æ‰“å°ä¸€æ¬¡loss
    learning_rate=2e-4,# å­¦ä¹ ç‡
    weight_decay=0.001,# æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    fp16=True,  # 3090 å®Œç¾æ”¯æŒ # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ŒåŠ é€Ÿå¹¶çœæ˜¾å­˜
    bf16=False, # ä¸ä½¿ç”¨bf16ï¼ˆ3090æ”¯æŒä¸ä½³ï¼‰
    max_grad_norm=0.3,# æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    max_steps=-1,# ä¸è®¾ç½®æœ€å¤§æ­¥æ•°ï¼Œç”±epochæ•°å†³å®š
    warmup_ratio=0.03,# å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
    group_by_length=True,# æŒ‰åºåˆ—é•¿åº¦åˆ†ç»„ï¼Œæé«˜å¡«å……æ•ˆç‡
    lr_scheduler_type="cosine",# å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šä½™å¼¦é€€ç«
    report_to="none",# ä¸æŠ¥å‘Šç»™ä»»ä½•å¹³å°ï¼ˆå¦‚W&Bï¼‰
    evaluation_strategy="steps",  # æ¯ N æ­¥è¯„ä¼°ä¸€æ¬¡# æŒ‰æ­¥æ•°è¿›è¡Œè¯„ä¼°
    eval_steps=100,  # æ¯ 100 æ­¥è¯„ä¼°
)
# In[]
## å¯åŠ¨è®­ç»ƒä¸ä¿å­˜

# åˆ›å»ºç›‘ç£å¾®è°ƒè®­ç»ƒå™¨
trainer = SFTTrainer(
    model=model,  # å·²é‡åŒ–çš„ã€å†»ç»“çš„ã€å‡†å¤‡å¥½çš„åŸºç¡€æ¨¡å‹
    train_dataset=dataset,# è®­ç»ƒæ•°æ®é›†
    eval_dataset=eval_dataset,# è¯„ä¼°æ•°æ®é›†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    peft_config=peft_parameters,# LoRA é…ç½®
    dataset_text_field="data",# æ•°æ®é›†ä¸­åŒ…å«æ–‡æœ¬çš„å­—æ®µå
    max_seq_length=1024, # æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé˜²æ­¢è¿‡é•¿æ ·æœ¬å¯¼è‡´OOM
    tokenizer=tokenizer,# åˆ†è¯å™¨
    args=training_params,# è®­ç»ƒå‚æ•°
    packing=False, # ä¸å°†å¤šä¸ªçŸ­æ ·æœ¬æ‰“åŒ…æˆä¸€ä¸ªé•¿æ ·æœ¬
)

# æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯ï¼ˆéªŒè¯LoRAæ˜¯å¦ç”Ÿæ•ˆï¼‰
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params}, æ€»å‚æ•°: {total_params}, æ¯”ä¾‹: {trainable_params/total_params:.2%}")

# æ­£å¼å¯åŠ¨è®­ç»ƒï¼
trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.model.save_pretrained(final_dir) # type: ignore # åªä¼šä¿å­˜ adapter_model.safetensors å’Œ adapter_config.json
trainer.tokenizer.save_pretrained(final_dir) # ä¿å­˜tokenizeré…ç½®

# æ¸…ç†æ˜¾å­˜
del trainer
torch.cuda.empty_cache()
print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {final_dir}")